# Feature Specification: Agentic RAG для рекомендаций вин

**Feature Branch**: `015-agentic-rag`
**Created**: 2026-02-10
**Status**: Draft
**Input**: User description: "Agentic RAG для винного сомелье-бота — замена случайной выборки 20 вин на интеллектуальный поиск через tool use"

## Clarifications

### Session 2026-02-10

- Q: Какие дополнительные поля Wine должны быть доступны для структурированного поиска помимо текущих 6 критериев? → A: Добавить food_pairings. Body/acidity/tannins покрываются семантическим поиском.
- Q: Применяется ли agentic RAG только к запросам пользователя или также к проактивным welcome-предложениям? → A: Только запросы пользователя (generate_response). Welcome-поток остаётся без изменений.
- Q: Заменяем ли 4-path маршрутизацию промптов (cold_start/personalized/event/food) на единый промпт с tool use? → A: Да, единый промпт + tool use. LLM сам определяет контекст и вызывает нужные инструменты.

## User Scenarios & Testing *(mandatory)*

### User Story 1 — Точный поиск по структурированным критериям (Priority: P1)

Пользователь пишет боту запрос с конкретными критериями: сорт винограда, тип вина, ценовой диапазон, страна, сладость, событие или блюдо. Система анализирует запрос, автоматически формирует структурированный запрос к базе данных и возвращает релевантные вина вместо случайной выборки.

**Why this priority**: Это ядро фичи. Текущая система передаёт в LLM случайные 20 вин из 50, из-за чего пользователь не получает релевантные рекомендации при конкретных запросах (например, "красное из Мальбека до 2000 рублей"). Без этой истории вся фича не имеет смысла.

**Independent Test**: Отправить боту сообщение "посоветуй красное вино из Мальбека до 2000 рублей" → получить в ответе только красные вина с сортом Мальбек в указанном ценовом диапазоне.

**Acceptance Scenarios**:

1. **Given** пользователь в чате с ботом, **When** пишет "хочу белое сухое вино до 1500 рублей", **Then** система формирует запрос с фильтрами (wine_type=white, sweetness=dry, price_max=1500) и рекомендует только подходящие вина
2. **Given** пользователь в чате, **When** пишет "есть что-нибудь из Пино Нуар?", **Then** система находит все вина с сортом Пино Нуар через фильтр по grape_varieties и рекомендует их
3. **Given** пользователь указывает несколько критериев, **When** пишет "розовое итальянское до 3000", **Then** система применяет все фильтры одновременно (wine_type=rose, country=Италия, price_max=3000)
4. **Given** пользователь пишет запрос с конкретными критериями, **When** в каталоге нет вин, удовлетворяющих всем фильтрам, **Then** система расширяет поиск (убирает наименее значимые фильтры) и предлагает ближайшие альтернативы с пояснением

---

### User Story 2 — Семантический поиск по описаниям и настроению (Priority: P2)

Пользователь описывает желаемое вино абстрактно: "что-то лёгкое и освежающее на лето", "элегантное вино для романтического ужина", "с нотами ванили и дуба". Система использует векторный поиск по описаниям и дегустационным заметкам для подбора наиболее подходящих вин.

**Why this priority**: Расширяет возможности поиска за пределы структурированных данных. Многие пользователи описывают желаемое вино через ощущения и контекст, а не через технические характеристики. Зависит от наличия корректных эмбеддингов в базе.

**Independent Test**: Отправить боту "хочу что-нибудь лёгкое и освежающее на лето" → получить вина с соответствующими описаниями (лёгкое тело, высокая кислотность, фруктовые ноты).

**Acceptance Scenarios**:

1. **Given** пользователь в чате, **When** пишет "что-нибудь элегантное к рыбе", **Then** система выполняет векторный поиск по описанию "элегантное вино к рыбе" и возвращает семантически близкие вина
2. **Given** пользователь описывает вкусовые предпочтения, **When** пишет "с нотами чёрной смородины и перца", **Then** система ищет по дегустационным заметкам и описаниям через векторное сходство
3. **Given** запрос содержит и структурированные, и абстрактные критерии, **When** пользователь пишет "элегантное красное до 3000", **Then** система комбинирует структурированный поиск (wine_type=red, price_max=3000) с семантическим ("элегантное")

---

### User Story 3 — Комбинированные и контекстные запросы (Priority: P3)

Пользователь ведёт диалог с ботом: уточняет предыдущий запрос, ссылается на контекст разговора. Система учитывает историю переписки при формировании поисковых запросов.

**Why this priority**: Улучшает пользовательский опыт в многошаговых диалогах, но основная ценность (точный поиск) уже реализована в US1 и US2.

**Independent Test**: Первое сообщение "посоветуй красное" → получить красные вина. Второе сообщение "а есть подешевле?" → получить красные вина с ценой ниже предыдущих рекомендаций.

**Acceptance Scenarios**:

1. **Given** бот рекомендовал 3 красных вина по 2000-3000 руб., **When** пользователь пишет "а подешевле?", **Then** система понимает контекст (красное + дешевле) и находит красные вина дешевле 2000 руб.
2. **Given** пользователь ведёт разговор о еде, **When** переключается на вопрос о вине "что к нему подать?", **Then** система учитывает упомянутое ранее блюдо при поиске

---

### Edge Cases

- Что происходит, когда ни один фильтр не извлечён из сообщения пользователя? Система возвращается к текущему поведению: передаёт разнообразную выборку вин из каталога
- Что происходит при пустом результате поиска? Система пробует расширить фильтры (убрать наименее значимые) или переключается на семантический поиск. Если всё равно пусто — сообщает пользователю и предлагает изменить критерии
- Что если LLM некорректно формирует параметры инструмента? Система валидирует параметры перед выполнением запроса; невалидные параметры игнорируются, используются только корректные
- Как обрабатывается тайм-аут при вызове инструмента? Если вызов инструмента не завершается за отведённое время, система использует fallback-выборку вин
- Что если LLM зацикливается в вызовах инструментов? Максимум 2 итерации tool calls; после этого система использует уже полученные результаты

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: Система MUST анализировать сообщение пользователя и автоматически определять, какой тип поиска вин применить (структурированный, семантический или оба)
- **FR-002**: Система MUST поддерживать структурированный поиск по следующим критериям: тип вина (red/white/rose/sparkling), сладость (dry/semi_dry/semi_sweet/sweet), сорт винограда (grape_varieties), ценовой диапазон (price_min/price_max), страна (country), регион (region), сочетаемость с блюдами (food_pairings)
- **FR-003**: Система MUST поддерживать семантический поиск по описаниям и дегустационным заметкам через существующий механизм векторного поиска
- **FR-004**: Система MUST ограничивать количество итераций вызова инструментов максимумом 2 за один запрос пользователя
- **FR-005**: Система MUST передавать в LLM только релевантные вина, найденные через инструменты, вместо случайной выборки из каталога
- **FR-006**: Система MUST сохранять обратную совместимость с существующим форматом ответов (маркеры [INTRO], [WINE:1-3], [CLOSING])
- **FR-007**: Система MUST использовать существующий формат каталога вин для промпта
- **FR-008**: Система MUST корректно обрабатывать случай, когда LLM не вызывает ни одного инструмента (fallback к текущему поведению)
- **FR-009**: Система MUST валидировать параметры, извлечённые LLM для инструментов, перед выполнением запроса к БД
- **FR-010**: Система MUST поддерживать комбинацию структурированного и семантического поиска в одном запросе
- **FR-011**: Система MUST логировать вызовы инструментов (название, параметры, количество найденных вин) для диагностики и мониторинга
- **FR-012**: Система MUST использовать единый системный промпт с описанием доступных инструментов вместо 4 раздельных промптов (cold_start, personalized, event, food_pairing). LLM самостоятельно определяет контекст запроса и выбирает инструменты

### Key Entities

- **Tool Call**: Запрос от LLM на выполнение инструмента. Содержит имя инструмента (search_wines / semantic_search) и параметры. Не персистится, существует только в рамках обработки одного запроса
- **Search Result**: Набор вин, найденных по запросу. Содержит список вин и метаданные поиска (какие фильтры применены, сколько найдено). Не персистится, передаётся в промпт LLM
- **Wine** (существующая): Используется как есть. Поля, по которым выполняется поиск: name, wine_type, sweetness, grape_varieties, price_rub, country, region, food_pairings, embedding, description, tasting_notes

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: При запросе с указанием сорта винограда (например, "Мальбек") 100% рекомендованных вин содержат этот сорт в grape_varieties (текущее состояние: ~40% из-за случайной выборки)
- **SC-002**: При запросе с ценовым диапазоном (например, "до 2000 рублей") 100% рекомендованных вин попадают в указанный диапазон
- **SC-003**: При абстрактном описании ("лёгкое освежающее") рекомендованные вина семантически релевантны запросу — пользователь оценивает релевантность как удовлетворительную в 80%+ случаев
- **SC-004**: Время ответа бота не увеличивается более чем на 3 секунды по сравнению с текущим (допускается 1-2 дополнительных вызова LLM для tool use)
- **SC-005**: Существующие сценарии (cold start, персонализация, события, еда) продолжают работать без деградации
- **SC-006**: Количество итераций tool calls не превышает 2 на один пользовательский запрос

## Scope

**В scope**: Поток обработки пользовательских запросов (`generate_response()`) — замена случайной выборки вин на интеллектуальный поиск через tool use.

**Вне scope**: Проактивный welcome-поток (`generate_welcome_with_suggestions()`) — остаётся без изменений.

## Assumptions

- Каталог вин остаётся небольшим (~50 вин), поэтому сложная оптимизация запросов не требуется
- Эмбеддинги уже сгенерированы для всех вин (фича 013 — T012)
- LLM-провайдер (через OpenRouter) поддерживает tool use / function calling
- Существующий формат ответов LLM ([INTRO], [WINE:1-3], [CLOSING]) не меняется
- Текущие механизмы detect_event() и detect_food() на основе ключевых слов и 4-path маршрутизация промптов (cold_start, personalized, event, food_pairing) заменяются единым промптом с tool use — LLM сам определяет контекст, когда и какие инструменты вызвать
- Пользователь общается на русском языке; инструменты принимают значения фильтров на английском (enum'ы) или русском (свободный текст)
