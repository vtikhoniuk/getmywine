# Research: Расширенные ответы на околовинные темы

## R-001: Причина кратких ответов на околовинные вопросы

**Decision**: Проблема в системном промпте — поле `intro` описано как «1-2 предложения», а для `response_type: "informational"` нет отдельных инструкций по объёму.

**Rationale**: В `SYSTEM_PROMPT_BASE` (строка 35):
```
"intro": "Вступительный текст (1-2 предложения)"
```
Это ограничение применяется ко всем типам ответов. Для `informational` нет исключения — LLM следует общей инструкции и даёт краткий ответ.

Кроме того, раздел «ПРАВИЛА ТИПОВ ОТВЕТА» (строка 48) описывает informational как:
```
"informational" — общий вопрос о вине без рекомендаций. wines пустой массив [].
```
Нет инструкции о развёрнутости ответа.

**Alternatives considered**:
- Изменить JSON-схему (добавить поле `body`) — отвергнуто: ломает обратную совместимость, требует изменений парсера, рендерера и Telegram-отправщика
- Использовать отдельный промпт для informational — отвергнуто: усложняет логику, сейчас тип определяется LLM на лету, а не заранее

## R-002: Оптимальное место для расширения инструкций

**Decision**: Добавить блок инструкций для `response_type: "informational"` в `SYSTEM_PROMPT_BASE`, в раздел «ПРАВИЛА ТИПОВ ОТВЕТА».

**Rationale**: Этот раздел уже определяет поведение для каждого типа ответа. Расширение инструкций здесь:
- Не нарушает структуру промпта
- Применяется ко всем вариантам промпта (cold start, personalized, agentic), т.к. все наследуют от BASE
- Не требует изменений в коде

**Alternatives considered**:
- Добавить отдельный `SYSTEM_PROMPT_INFORMATIONAL` — отвергнуто: тип ответа определяется LLM в момент генерации, а не до неё
- Изменить описание поля `intro` — отвергнуто: для recommendation 1-2 предложения корректны

## R-003: Как использовать поле `intro` для развёрнутого ответа

**Decision**: Для `response_type: "informational"` поле `intro` содержит основной развёрнутый ответ (5-10 предложений), а `closing` содержит тематическую подводку к обсуждению вин.

**Rationale**: JSON-схема `SommelierResponse` уже достаточна:
- `intro` — основной текст (для informational: развёрнутый ответ)
- `wines` — пустой массив `[]` (для informational)
- `closing` — вопрос/подводка (для informational: тематическая подводка к конкретным винам)

Не нужно менять схему. Достаточно в промпте указать, что для informational поле `intro` должно содержать 5-10 предложений.

`render_response_text()` уже корректно обрабатывает этот случай: объединяет `intro` + `closing` через `\n\n`.

**Alternatives considered**:
- Добавить поле `body` в схему — отвергнуто (см. R-001)
- Использовать `description` в wines — отвергнуто: wines должен быть пустым для informational

## R-004: Langfuse-тегирование informational ответов

**Decision**: Добавить тег `response_type` в метаданные Langfuse-наблюдения после парсинга структурированного ответа.

**Rationale**: Текущий `_update_langfuse_metadata()` передаёт `tools_used` и `iterations`. Добавление `response_type` минимально и позволяет фильтровать informational ответы в дашборде Langfuse.

Место добавления — метод `generate_agentic_response()` в `sommelier.py`, после парсинга JSON-ответа, где уже извлекается `response_type`.

**Alternatives considered**:
- Тегировать на уровне `ChatService` — отвергнуто: `ChatService` не имеет доступа к `langfuse_context`, и парсинг ответа происходит для других целей (guard detection)
- Создать отдельный `@observe()` декоратор — отвергнуто: избыточно для одного поля метаданных

## R-005: Стратегия тестирования

**Decision**: Eval-тесты (LLM-вызовы) в `backend/tests/eval/test_informational_eval.py`.

**Rationale**:
- Unit-тесты для промптов бессмысленны — строковое содержимое промпта не тестируется юнитами
- Eval-тесты отправляют реальные запросы к LLM и проверяют структуру/качество ответа
- Существующий паттерн: `test_structured_output_eval.py` уже тестирует informational queries, но без проверки длины ответа

Тестовые сценарии:
1. Околовинный вопрос → `response_type == "informational"`, `intro` содержит >= 3 предложений
2. Околовинный вопрос → `closing` содержит тематическую подводку (ключевые слова о вине)
3. 3 разных околовинных вопроса → `closing` отличается
4. Регрессия: вопрос о конкретном вине → по-прежнему `response_type == "recommendation"`

**Alternatives considered**:
- Только ручное тестирование — отвергнуто: нет автоматизированной проверки, противоречит TDD
- Мок-тесты с фиксированными ответами — отвергнуто: не проверяют реальное поведение LLM с обновлённым промптом
