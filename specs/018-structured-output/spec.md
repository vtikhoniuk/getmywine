# Feature Specification: Structured Output for Wine Recommendations

**Feature Branch**: `018-structured-output`
**Created**: 2026-02-11
**Status**: Draft
**Input**: User description: "Переделать работу LLM на ответы на основе structured output. Если нужно — можно поменять модель, в т.ч. отказаться от OpenRouter. Исследовать, какие модели хорошо выполняют tool calling и structured output."

## Context & Problem Statement

Текущая система отправки винных рекомендаций в Telegram зависит от того, что LLM генерирует текстовые маркеры `[INTRO]`, `[WINE:1]`, `[CLOSING]` в свободном тексте. Парсер извлекает секции регулярными выражениями. Эта схема ненадёжна:

- GPT-4.1 (текущая модель) не поддерживает `response_format: json_schema` — маркеры генерируются нестабильно
- При отсутствии маркеров бот отправляет весь текст одним сообщением, без фотографий вин
- Извлечение вин из ответа происходит через поиск имён в тексте — хрупкий механизм
- Два пути отправки (structured/fallback) усложняют код и создают непредсказуемое поведение

**Structured Output** — функция API, которая заставляет модель возвращать ответ строго по заданной JSON-схеме. Это гарантирует структуру ответа и устраняет необходимость в текстовом парсинге.

## Research: Model Capabilities

| Модель | Structured Output (`response_format`) | Tool Calling | Оба одновременно | Примечания |
|--------|---------------------------------------|-------------|-------------------|------------|
| **GPT-4.1** (текущая) | Не поддерживает | Да | Нет | Structured output только через `strict: true` на tools |
| **GPT-4o / 4o-mini** | Да (`json_schema`) | Да | Частично | `response_format` игнорируется при tool calls, применяется только к финальному текстовому ответу |
| **Claude Sonnet 4.5 / Opus 4.6** | Да (`output_config.format`) | Да | **Да, оба полноценно** | Единственный провайдер с документированной комбинацией tools + structured output |
| **Gemini 2.5/3** | Да (`response_schema`) | Да | Да (Gemini 3) | Полная поддержка JSON Schema |

**OpenRouter**: поддерживает проброс `response_format: json_schema` — автоматически маппит на нативный параметр Claude (`output_config.format`). Флаг `provider.require_parameters: true` гарантирует маршрутизацию на провайдер с поддержкой.

**Langfuse**: обёртка `langfuse.openai.AsyncOpenAI` прозрачно прокидывает `response_format` — изменений в интеграции не требуется.

## User Scenarios & Testing

### User Story 1 — Винная рекомендация с фотографиями (Priority: P1)

Пользователь отправляет сообщение в Telegram-бот с запросом на вино (например, "красное к стейку"). Бот отвечает серией из 5 отдельных сообщений: приветствие, 3 карточки вин с фотографиями и подписями, завершающий вопрос.

**Why this priority**: Это основной пользовательский сценарий. Без надёжной структуры ответа бот отправляет нечитаемый блок текста без фотографий — главная жалоба пользователей.

**Independent Test**: Отправить боту "посоветуй вино к рыбе" и проверить, что ответ приходит в виде 5 отдельных сообщений с фотографиями.

**Acceptance Scenarios**:

1. **Given** пользователь отправляет запрос на вино, **When** LLM возвращает рекомендацию, **Then** бот отправляет 5 отдельных сообщений: intro, 3 карточки вин с фото, closing
2. **Given** пользователь отправляет запрос на вино, **When** LLM не находит подходящих вин в каталоге, **Then** бот отправляет сообщение с intro, объяснением ситуации и closing — без пустых карточек
3. **Given** пользователь отправляет запрос на вино, **When** LLM находит только 1-2 подходящих вина, **Then** бот отправляет сообщения только для найденных вин (не заполняет до 3)

---

### User Story 2 — Приветственное сообщение /start (Priority: P1)

Пользователь нажимает /start. Бот отправляет приветствие с 3 проактивными предложениями вин на основе текущего контекста (события, сезон, время суток), каждое с фотографией.

**Why this priority**: Первый контакт с ботом определяет пользовательский опыт. Приветствие без фотографий выглядит незавершённым.

**Independent Test**: Нажать /start в боте и проверить, что приходят 5 сообщений с фотографиями.

**Acceptance Scenarios**:

1. **Given** новый пользователь нажимает /start, **When** бот генерирует приветствие, **Then** ответ приходит в виде 5 отдельных сообщений: intro, 3 вина с фото, closing
2. **Given** пользователь нажимает /start, **When** у подобранных вин нет изображений, **Then** карточки вин отправляются как текстовые сообщения без фото

---

### User Story 3 — Общий вопрос о вине без рекомендаций (Priority: P2)

Пользователь задаёт образовательный вопрос (например, "что такое танины?"). Бот отвечает информативным сообщением без винных карточек.

**Why this priority**: Бот должен различать запросы на рекомендации и общие вопросы, не пытаясь "вставить" винные карточки туда, где они не нужны.

**Independent Test**: Спросить бота "расскажи про пино нуар" и проверить, что ответ пришёл одним текстовым сообщением.

**Acceptance Scenarios**:

1. **Given** пользователь задаёт общий вопрос, **When** LLM отвечает без рекомендаций вин, **Then** бот отправляет ответ одним текстовым сообщением
2. **Given** пользователь задаёт вопрос о сорте, **When** LLM решает предложить вина из каталога, **Then** ответ приходит в структурированном формате с карточками

---

### User Story 4 — Off-topic / prompt injection запросы (Priority: P2)

Пользователь отправляет нерелевантный запрос ("реши уравнение") или пытается манипулировать ботом. Бот вежливо отклоняет и предлагает помощь с вином. Тип защиты (off_topic, prompt_injection, social_engineering) отслеживается в ответе.

**Why this priority**: Защитное поведение должно работать независимо от формата ответа.

**Independent Test**: Отправить "реши уравнение x^2 = 4" и проверить, что бот перенаправляет к теме вина.

**Acceptance Scenarios**:

1. **Given** пользователь задаёт off-topic вопрос, **When** LLM генерирует ответ, **Then** ответ содержит тип защиты (off_topic) и перенаправление к вину с рекомендациями из каталога

---

### Edge Cases

- Что происходит, если LLM возвращает JSON с именем вина, которого нет в каталоге?
- Как система обрабатывает таймаут или ошибку LLM при structured output?
- Что происходит при переключении модели в конфигурации на модель без поддержки structured output?
- Как обрабатывается случай, когда LLM возвращает 0 вин в structured response?
- Как система ведёт себя, если LLM возвращает невалидный JSON (нарушение схемы)?

## Requirements

### Functional Requirements

- **FR-001**: Система ДОЛЖНА использовать structured output (JSON schema) для получения ответов LLM с рекомендациями вин вместо текстовых маркеров
- **FR-002**: Ответ LLM ДОЛЖЕН содержать отдельные поля: вступление, список вин (с именем, описанием и объяснением), завершающий вопрос
- **FR-003**: Каждое вино в ответе ДОЛЖНО содержать имя, точно совпадающее с каталогом (LLM выбирает из результатов поиска), и текстовое объяснение выбора
- **FR-004**: Система ДОЛЖНА поддерживать два типа ответов: рекомендация вин (structured с карточками) и информационный ответ (свободный текст), различаемые полем типа в JSON-ответе
- **FR-005**: Agentic flow (tool calling + финальный ответ) ДОЛЖЕН сохранить возможность вызова инструментов поиска (search_wines, semantic_search) и возвращать structured response в финальном ответе
- **FR-006**: Система ДОЛЖНА поддерживать маркер защиты (guard type) в structured ответе для off-topic и prompt injection случаев
- **FR-007**: Система ДОЛЖНА поддерживать переменное количество вин в ответе (0-3) в зависимости от результатов поиска
- **FR-008**: Система ДОЛЖНА работать как с OpenRouter (с пробросом response_format), так и с прямым API модели для structured output
- **FR-009**: Отправка сообщений в Telegram ДОЛЖНА использовать структурированный ответ напрямую, без текстового парсинга
- **FR-010**: Текущий heuristic fallback-парсер ДОЛЖЕН быть сохранён как резервный механизм на случай получения неструктурированного ответа (переходный период или fallback-модели)
- **FR-011**: В историю диалога ДОЛЖЕН сохраняться рендеренный текст (собранный из полей JSON-ответа), а не сырой JSON — для экономии токенов и естественного контекста при продолжении диалога

### Key Entities

- **StructuredRecommendation**: Ответ LLM с рекомендациями — содержит вступление (текст), список вин (0-3 штуки, каждое с именем и объяснением), завершающий вопрос (текст), опциональный тип защиты
- **InformationalResponse**: Ответ LLM без рекомендаций — содержит свободный текст ответа и опциональный тип защиты
- **WineReference**: Ссылка на вино внутри рекомендации — имя из каталога и контекстное объяснение выбора для пользователя

## Success Criteria

### Measurable Outcomes

- **SC-001**: 100% ответов с рекомендациями вин отображаются в Telegram как отдельные сообщения с фотографиями (вместо текущих ~50% без маркеров)
- **SC-002**: Время ответа бота (от сообщения пользователя до последнего сообщения бота) не увеличивается более чем на 500мс по сравнению с текущим
- **SC-003**: Количество кода, связанного с парсингом текста (regex, heuristic parsing), сокращается минимум на 50%
- **SC-004**: Все существующие сценарии (welcome, recommendation, off-topic, guard) продолжают работать без регрессий
- **SC-005**: Переключение между моделями с поддержкой structured output не требует изменений в коде отправки сообщений

## Assumptions

- Claude Sonnet 4.5 через OpenRouter (`anthropic/claude-sonnet-4`) используется как основная модель (поддерживает structured output + tool calling одновременно); при необходимости может быть заменена на другую модель с аналогичной поддержкой
- OpenRouter продолжает поддерживать проброс `response_format` для Claude-моделей
- Langfuse wrapper остаётся совместимым с параметром `response_format`
- Существующие инструменты поиска вин (search_wines, semantic_search) не требуют изменений
- Схема базы данных вин не меняется
- Формат отправки сообщений в Telegram (5 отдельных сообщений) не меняется

## Clarifications

### Session 2026-02-11

- Q: Какую модель использовать как основную для structured output + tool calling? → A: Claude Sonnet 4.5 через OpenRouter (`anthropic/claude-sonnet-4`), с возможностью замены позже
- Q: Что сохранять в историю диалога при JSON-ответах LLM? → A: Рендеренный текст (собранный из полей JSON: вступление + описания вин + завершение), а не сырой JSON
